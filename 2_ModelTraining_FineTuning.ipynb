{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.10.14\n",
    "# Below works only on Python > 3.9 \n",
    "# pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 --upgrade --force-reinstall\n",
    "# pip show torh # 2.5.1+cu121\n",
    "# pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# pip install pandas matplotlib pillow tqdm \n",
    "# pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    !gdown https://drive.google.com/uc?id=19En9_JPWpdp9NlZIMV6hmIgOW4Uti4y8\n",
    "\n",
    "    import shutil\n",
    "    shutil.unpack_archive('input.zip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from PIL import Image\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from unsloth import FastVisionModel\n",
    "import torch\n",
    "\n",
    "pd.options.display.max_rows = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image and Text data for Visual-Language LLM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Normal</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130445</th>\n",
       "      <td>Ventricular-Normal Fusion</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130446</th>\n",
       "      <td>Ventricular-Normal Fusion</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130447 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            class image\n",
       "0                          Normal  None\n",
       "1                          Normal  None\n",
       "...                           ...   ...\n",
       "130445  Ventricular-Normal Fusion  None\n",
       "130446  Ventricular-Normal Fusion  None\n",
       "\n",
       "[130447 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_text_train = 'input/mitbih_train_resampled.csv'\n",
    "path_text_test  = 'input/mitbih_test_renamed.csv'\n",
    "\n",
    "df_train = pd.read_csv(path_text_train, usecols=['class'])\n",
    "df_test  = pd.read_csv(path_text_test, usecols=['class']) \n",
    "\n",
    "df_train['image'] = None\n",
    "df_test['image']  = None\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/input/images/train/3.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m image_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path_images_train, image_file)\n\u001b[0;32m----> 7\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m image_rgb \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# Grey scale\u001b[39;00m\n\u001b[1;32m      9\u001b[0m df_train\u001b[38;5;241m.\u001b[39mloc[index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m image_rgb\n",
      "File \u001b[0;32m/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/PIL/Image.py:3469\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3466\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3469\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3470\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/input/images/train/3.png'"
     ]
    }
   ],
   "source": [
    "path_images_train = 'input/images/train'\n",
    "path_images_test  = 'input/images/test'\n",
    "\n",
    "for index in df_train.index:\n",
    "    image_file = f'{index}.png'\n",
    "    image_path = os.path.join(path_images_train, image_file)\n",
    "    image = Image.open(image_path)\n",
    "    image_rgb = image.convert('L') # Grey scale\n",
    "    df_train.loc[index, 'image'] = image_rgb\n",
    "\n",
    "for index in df_test.index:\n",
    "    image_file = f'{index}.png'\n",
    "    image_path = os.path.join(path_images_test, image_file)\n",
    "    image = Image.open(image_path)\n",
    "    image_rgb = image.convert('L') # Grey scale\n",
    "    df_test.loc[index, 'image'] = image_rgb    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=L size=600x400 at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Normal</td>\n",
       "      <td>&lt;PIL.Image.Image image mode=L size=600x400 at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130445</th>\n",
       "      <td>Ventricular-Normal Fusion</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130446</th>\n",
       "      <td>Ventricular-Normal Fusion</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130447 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            class  \\\n",
       "0                          Normal   \n",
       "1                          Normal   \n",
       "...                           ...   \n",
       "130445  Ventricular-Normal Fusion   \n",
       "130446  Ventricular-Normal Fusion   \n",
       "\n",
       "                                                    image  \n",
       "0       <PIL.Image.Image image mode=L size=600x400 at ...  \n",
       "1       <PIL.Image.Image image mode=L size=600x400 at ...  \n",
       "...                                                   ...  \n",
       "130445                                               None  \n",
       "130446                                               None  \n",
       "\n",
       "[130447 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_user = \"\"\"\n",
    "You are a specialist Cardiologist specializing in the analysis of ECG (Electrocardiogram) signals.\n",
    "Your goal is to analyze ECG signals and classify them accurately.\n",
    "You are provided with a ECG image and a class that corresponds to the image.\n",
    "You only classify ECG signals into one of the following five categories: \n",
    "\"Normal\", \"Supraventricular Arrhythmia\", \"Ventricular Arrhythmia\", \"Ventricular-Normal Fusion\", \"Paced-Normal Fusion\".\n",
    "Each ECG signal must be classified into exactly one of these categories.\n",
    "No additional text should be included in the output.\n",
    "You are provided with an ECG signal image to look into. It is Time (x-axis) vs Voltage (y-axis). \n",
    "Classify the ECG signal accurately by analyzing the provided data.\n",
    "\"\"\"\n",
    "\n",
    "def conversations(row):\n",
    "\n",
    "   conversation = [ {  \"role\": \"user\",\n",
    "                     \"content\": [ {\"type\": \"text\", \"text\": instruction_user},\n",
    "                                    {\"type\": \"image\", \"image\": row[\"image\"]},],},\n",
    "\n",
    "                    {  \"role\": \"assistant\",\n",
    "                     \"content\": [{\"type\": \"text\", \"text\": row['class']}]}]\n",
    "\n",
    "   return {\"messages\": conversation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = [  conversations(row) for _, row in df_train.iterrows() ]\n",
    "data_test  = [  conversations(row) for _, row in df_test.iterrows()  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': '\\nYou are a specialist Cardiologist specializing in the analysis of ECG (Electrocardiogram) signals.\\nYour goal is to analyze ECG signals and classify them accurately.\\nYou are provided with a ECG image and a class that corresponds to the image.\\nYou only classify ECG signals into one of the following five categories: \\n\"Normal\", \"Supraventricular Arrhythmia\", \"Ventricular Arrhythmia\", \"Ventricular-Normal Fusion\", \"Paced-Normal Fusion\".\\nEach ECG signal must be classified into exactly one of these categories.\\nNo additional text should be included in the output.\\nYou are provided with an ECG signal image to look into. It is Time (x-axis) vs Voltage (y-axis). \\nClassify the ECG signal accurately by analyzing the provided data.\\n'},\n",
       "    {'type': 'image', 'image': <PIL.Image.Image image mode=L size=600x400>}]},\n",
       "  {'role': 'assistant', 'content': [{'type': 'text', 'text': 'Normal'}]}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_model = \"Qwen2-VL-7B-Instruct-bnb-4bit\" \n",
    "\n",
    "model, tokenizer = FastVisionModel.from_pretrained( \"unsloth/\" + name_model, \n",
    "                                                   load_in_4bit = True,\n",
    "                                                   use_gradient_checkpointing = \"unsloth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the PEFT (Parameter-Efficient Fine-Tuning) includes LoRA (Low-Rank Adaptation) \n",
    "\n",
    "model = FastVisionModel.get_peft_model(\n",
    "            model,\n",
    "            finetune_vision_layers    = True,\n",
    "            finetune_language_layers  = True,\n",
    "            finetune_attention_modules= True,\n",
    "            finetune_mlp_modules      = True,\n",
    "            r = 16,\n",
    "            lora_alpha = 16,\n",
    "            lora_dropout = 0,\n",
    "            bias = \"none\",\n",
    "            random_state = 3443,\n",
    "            use_rslora = False,\n",
    "            loftq_config = None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import is_bf16_supported # 16-bit fl-point format to improve performance/memory\n",
    "from unsloth.trainer import UnslothVisionDataCollator # preparing/collating data for vision tasks\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Enable \"model\" for training\n",
    "FastVisionModel.for_training(model)  \n",
    "\n",
    "trainer = SFTTrainer(model=model, tokenizer=tokenizer, \n",
    "                     data_collator=UnslothVisionDataCollator(model, tokenizer),  \n",
    "                     train_dataset=data_train, \n",
    "                     args=SFTConfig(per_device_train_batch_size=2,\n",
    "                                    gradient_accumulation_steps=4,\n",
    "                                    warmup_steps=5,\n",
    "                                    #max_steps=60,       # either this or num_train_epochs\n",
    "                                    num_train_epochs=1,  # full pass over your dataset [1:3 max]\n",
    "                                    learning_rate=2e-4,\n",
    "                                    fp16=not is_bf16_supported(),\n",
    "                                    bf16=is_bf16_supported(),\n",
    "                                    logging_steps=5,\n",
    "                                    optim=\"adamw_8bit\",\n",
    "                                    weight_decay=0.01,\n",
    "                                    lr_scheduler_type=\"linear\",\n",
    "                                    seed=3407,\n",
    "                                    output_dir=\"outputs\",\n",
    "                                    report_to=\"none\",  # for weights and biases\n",
    "                                    remove_unused_columns=False,\n",
    "                                    dataset_text_field=\"\",\n",
    "                                    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "                                    dataset_num_proc=4,\n",
    "                                    max_seq_length=2048))\n",
    "\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer locally\n",
    "\n",
    "name_finetuned = name_model + \"_finetuned_\" + \"ecg\"\n",
    "\n",
    "model.save_pretrained(name_finetuned) \n",
    "tokenizer.save_pretrained(name_finetuned)\n",
    "\n",
    "import shutil\n",
    "shutil.make_archive(name_finetuned, 'zip', name_finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push models to the Hugging Face Hub\n",
    "\n",
    "if False: \n",
    "    # !pip install --upgrade huggingface_hub\n",
    "    from huggingface_hub import notebook_login\n",
    "    notebook_login()\n",
    "\n",
    "model.push_to_hub( \"Aidan777/\"    + name_finetuned) \n",
    "tokenizer.push_to_hub(\"Aidan777/\" + name_finetuned) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
