{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.10.14\n",
    "# Below works only on Python > 3.9 \n",
    "# pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 --upgrade --force-reinstall\n",
    "# pip show torch # 2.5.1+cu121\n",
    "# pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "# pip install pandas matplotlib pillow tqdm \n",
    "# pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    !gdown https://drive.google.com/uc?id=19En9_JPWpdp9NlZIMV6hmIgOW4Uti4y8\n",
    "\n",
    "    import shutil\n",
    "    shutil.unpack_archive('input.zip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from PIL import Image\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from unsloth import FastVisionModel\n",
    "import torch\n",
    "import joblib\n",
    "pd.options.display.max_rows = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image and Text data for Visual-Language LLM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Normal</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130445</th>\n",
       "      <td>Ventricular-Normal Fusion</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130446</th>\n",
       "      <td>Ventricular-Normal Fusion</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130447 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            class image\n",
       "0                          Normal  None\n",
       "1                          Normal  None\n",
       "...                           ...   ...\n",
       "130445  Ventricular-Normal Fusion  None\n",
       "130446  Ventricular-Normal Fusion  None\n",
       "\n",
       "[130447 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_text_train = 'input/mitbih_train_resampled.csv'\n",
    "path_text_test  = 'input/mitbih_test_renamed.csv'\n",
    "\n",
    "df_train = pd.read_csv(path_text_train, usecols=['class'])\n",
    "df_test  = pd.read_csv(path_text_test, usecols=['class']) \n",
    "\n",
    "df_train['image'] = None\n",
    "df_test['image']  = None\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672b8300b6aa40178c156588a399a595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/130447 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_images_train = 'input/images/train'\n",
    "\n",
    "def process_image(index, path_images):\n",
    "    image_file = f'{index}.png'\n",
    "    image_path = os.path.join(path_images, image_file)\n",
    "    try: \n",
    "        image = Image.open(image_path)\n",
    "        image.resize((300, 200))\n",
    "        image_rgb = image.convert('L')  # Grey scale\n",
    "        image.close() \n",
    "    except:\n",
    "        image_rgb = None\n",
    "    return index, image_rgb\n",
    "    \n",
    "results_train = joblib.Parallel(n_jobs=20)(joblib.delayed(process_image)(index, path_images_train) for \n",
    "                                           index in tqdm(df_train.index))\n",
    "\n",
    "image_dict_train = {index: image_rgb for index, image_rgb in results_train}\n",
    "df_train['image'] = df_train.index.map(image_dict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_images_test = 'input/images/test'\n",
    "\n",
    "results_test = joblib.Parallel(n_jobs=-1)(joblib.delayed(process_image)(index, path_images_test) for\n",
    "                                          index in tqdm(df_test.index))\n",
    "\n",
    "image_dict_test = {index: image_rgb for index, image_rgb in results_test}\n",
    "df_test['image'] = df_test.index.map(image_dict_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_user = \"\"\"\n",
    "You are a specialist Cardiologist specializing in the analysis of ECG (Electrocardiogram) signals.\n",
    "Your goal is to analyze ECG signals and classify them accurately.\n",
    "You are provided with a ECG image and a class that corresponds to the image.\n",
    "You only classify ECG signals into one of the following five categories: \n",
    "\"Normal\", \"Supraventricular Arrhythmia\", \"Ventricular Arrhythmia\", \"Ventricular-Normal Fusion\", \"Paced-Normal Fusion\".\n",
    "Each ECG signal must be classified into exactly one of these categories.\n",
    "No additional text should be included in the output.\n",
    "You are provided with an ECG signal image to look into. It is Time (x-axis) vs Voltage (y-axis). \n",
    "Classify the ECG signal accurately by analyzing the provided data.\n",
    "\"\"\"\n",
    "\n",
    "def conversations(row):\n",
    "\n",
    "   conversation = [ {  \"role\": \"user\",\n",
    "                     \"content\": [ {\"type\": \"text\", \"text\": instruction_user},\n",
    "                                    {\"type\": \"image\", \"image\": row[\"image\"]},],},\n",
    "\n",
    "                    {  \"role\": \"assistant\",\n",
    "                     \"content\": [{\"type\": \"text\", \"text\": row['class']}]}]\n",
    "\n",
    "   return {\"messages\": conversation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = [  conversations(row) for _, row in df_train.iterrows() ]\n",
    "data_test  = [  conversations(row) for _, row in df_test.iterrows()  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': '\\nYou are a specialist Cardiologist specializing in the analysis of ECG (Electrocardiogram) signals.\\nYour goal is to analyze ECG signals and classify them accurately.\\nYou are provided with a ECG image and a class that corresponds to the image.\\nYou only classify ECG signals into one of the following five categories: \\n\"Normal\", \"Supraventricular Arrhythmia\", \"Ventricular Arrhythmia\", \"Ventricular-Normal Fusion\", \"Paced-Normal Fusion\".\\nEach ECG signal must be classified into exactly one of these categories.\\nNo additional text should be included in the output.\\nYou are provided with an ECG signal image to look into. It is Time (x-axis) vs Voltage (y-axis). \\nClassify the ECG signal accurately by analyzing the provided data.\\n'},\n",
       "    {'type': 'image', 'image': <PIL.Image.Image image mode=L size=600x400>}]},\n",
       "  {'role': 'assistant', 'content': [{'type': 'text', 'text': 'Normal'}]}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_model = \"Qwen2-VL-7B-Instruct-bnb-4bit\" \n",
    "\n",
    "model, tokenizer = FastVisionModel.from_pretrained( \"unsloth/\" + name_model, \n",
    "                                                   load_in_4bit = True,\n",
    "                                                   use_gradient_checkpointing = \"unsloth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the PEFT (Parameter-Efficient Fine-Tuning) includes LoRA (Low-Rank Adaptation) \n",
    "\n",
    "model = FastVisionModel.get_peft_model(\n",
    "            model,\n",
    "            finetune_vision_layers    = True,\n",
    "            finetune_language_layers  = True,\n",
    "            finetune_attention_modules= True,\n",
    "            finetune_mlp_modules      = True,\n",
    "            r = 16,\n",
    "            lora_alpha = 16,\n",
    "            lora_dropout = 0,\n",
    "            bias = \"none\",\n",
    "            random_state = 3443,\n",
    "            use_rslora = False,\n",
    "            loftq_config = None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import is_bf16_supported # 16-bit fl-point format to improve performance/memory\n",
    "from unsloth.trainer import UnslothVisionDataCollator # preparing/collating data for vision tasks\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Enable \"model\" for training\n",
    "FastVisionModel.for_training(model)  \n",
    "\n",
    "trainer = SFTTrainer(model=model, tokenizer=tokenizer, \n",
    "                     data_collator=UnslothVisionDataCollator(model, tokenizer),  \n",
    "                     train_dataset=data_train, \n",
    "                     args=SFTConfig(per_device_train_batch_size=2,\n",
    "                                    gradient_accumulation_steps=4,\n",
    "                                    warmup_steps=5,\n",
    "                                    #max_steps=60,       # either this or num_train_epochs\n",
    "                                    num_train_epochs=1,  # full pass over your dataset [1:3 max]\n",
    "                                    learning_rate=2e-4,\n",
    "                                    fp16=not is_bf16_supported(),\n",
    "                                    bf16=is_bf16_supported(),\n",
    "                                    logging_steps=5,\n",
    "                                    optim=\"adamw_8bit\",\n",
    "                                    weight_decay=0.01,\n",
    "                                    lr_scheduler_type=\"linear\",\n",
    "                                    seed=3407,\n",
    "                                    output_dir=\"outputs\",\n",
    "                                    report_to=\"none\",  # for weights and biases\n",
    "                                    remove_unused_columns=False,\n",
    "                                    dataset_text_field=\"\",\n",
    "                                    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "                                    dataset_num_proc=4,\n",
    "                                    max_seq_length=2048))\n",
    "\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and tokenizer locally\n",
    "\n",
    "name_finetuned = name_model + \"_finetuned_\" + \"ecg\"\n",
    "\n",
    "model.save_pretrained(name_finetuned) \n",
    "tokenizer.save_pretrained(name_finetuned)\n",
    "\n",
    "import shutil\n",
    "shutil.make_archive(name_finetuned, 'zip', name_finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push models to the Hugging Face Hub\n",
    "\n",
    "if False: \n",
    "    # !pip install --upgrade huggingface_hub\n",
    "    from huggingface_hub import notebook_login\n",
    "    notebook_login()\n",
    "\n",
    "model.push_to_hub( \"Aidan777/\"    + name_finetuned) \n",
    "tokenizer.push_to_hub(\"Aidan777/\" + name_finetuned) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv3.10.14",
   "language": "python",
   "name": "myenv3.10.14"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
